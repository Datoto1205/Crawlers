{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice of Regression\n",
    "* The tutorial website I refered to: \n",
    "- http://to52016.pixnet.net/blog/post/343518241-%5Bpython%5D--機器學習%28scikit-learn%29---簡單回歸分析\n",
    "- https://blog.csdn.net/u012162613/article/details/42192293\n",
    "\n",
    "\n",
    "### Load The Data\n",
    "* The code in this part was extracted from the file of \"Practice of Support Vector Machine.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n",
      "1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n",
      "2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n",
      "3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n",
      "4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n",
      "5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n",
      "6  0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n",
      "7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n",
      "8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n",
      "9  0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  \n",
      "0     15.3  396.90   4.98  \n",
      "1     17.8  396.90   9.14  \n",
      "2     17.8  392.83   4.03  \n",
      "3     18.7  394.63   2.94  \n",
      "4     18.7  396.90   5.33  \n",
      "5     18.7  394.12   5.21  \n",
      "6     15.2  395.60  12.43  \n",
      "7     15.2  396.90  19.15  \n",
      "8     15.2  386.63  29.93  \n",
      "9     15.2  386.71  17.10  \n",
      "   Prediction of Price\n",
      "0                 24.0\n",
      "1                 21.6\n",
      "2                 34.7\n",
      "3                 33.4\n",
      "4                 36.2\n",
      "5                 28.7\n",
      "6                 22.9\n",
      "7                 27.1\n",
      "8                 16.5\n",
      "9                 18.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>Prediction of Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n",
       "5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n",
       "6  0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n",
       "7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n",
       "8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n",
       "9  0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  Prediction of Price  \n",
       "0     15.3  396.90   4.98                 24.0  \n",
       "1     17.8  396.90   9.14                 21.6  \n",
       "2     17.8  392.83   4.03                 34.7  \n",
       "3     18.7  394.63   2.94                 33.4  \n",
       "4     18.7  396.90   5.33                 36.2  \n",
       "5     18.7  394.12   5.21                 28.7  \n",
       "6     15.2  395.60  12.43                 22.9  \n",
       "7     15.2  396.90  19.15                 27.1  \n",
       "8     15.2  386.63  29.93                 16.5  \n",
       "9     15.2  386.71  17.10                 18.9  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "print(boston['feature_names'])\n",
    "\n",
    "\n",
    "# Extract all the elements in 'data' type and create a dataframe with module \"pandas\".\n",
    "dataOfFW = pd.DataFrame(boston['data'], columns = boston['feature_names'])\n",
    "print(dataOfFW.head(10))    # Show top 10 rows of dataOfFW.\n",
    "\n",
    "\n",
    "# Extract all the elements in 'target' type and create a dataframe with module \"pandas\".\n",
    "resultOfFW = pd.DataFrame(boston['target'], columns = ['Prediction of Price'])\n",
    "print(resultOfFW.head(10))    # Show top 10 rows of resultOfFW.\n",
    "\n",
    "# Combine the data form and result form\n",
    "newDF = pd.concat([dataOfFW, resultOfFW], axis = 1)\n",
    "newDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Training Sample Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We use this function to create train set and test set randomly.\n",
    "X_train,X_test, y_train, y_test = train_test_split(newDF[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX',\t'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']], newDF[['Prediction of Price']], test_size = 0.3, random_state = 0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize The Data of Sample Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "# I need to instantiate StandardScaler first, or I would get some errors.\n",
    "\n",
    "sc.fit(X_train)                        # Initialize the action of standardizing the feature set of sample.\n",
    "X_train_nor = sc.transform(X_train)    # Get the standardized value of train sample set.\n",
    "X_test_nor = sc.transform(X_test)      # Get the standardized value of test sample set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate The Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each coefficient in the above regression model: [[-0.99884541  1.04926169  0.08283818  0.61938203 -1.87828363  2.69828142\n",
      "  -0.27592019 -3.09716434  2.09139628 -1.88530946 -2.26382523  0.59446503\n",
      "  -3.45020002]]\n",
      "The intercept in the above regression model: [22.74548023]\n",
      "The R-square of the above regression model: 0.7644563391821222\n",
      "The adjusted R-square of the above regression model: 0.7554502580332033\n",
      "The p-value of each coefficient in the above regression model: [6.41158158e-14 4.95302857e-14 3.05628798e-24 2.17666529e-03\n",
      " 4.28001007e-17 2.44958160e-55 2.24917643e-14 2.90986820e-06\n",
      " 2.83596692e-14 6.28935737e-22 3.28375250e-31 4.03681568e-10\n",
      " 1.99312376e-63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# Declare & Initialize the action of linear regression.\n",
    "regression = linear_model.LinearRegression()\n",
    "\n",
    "# Train the Model\n",
    "regression.fit(X_train_nor, y_train)   \n",
    "\n",
    "# Examine The Performance of Linear Regression Model\n",
    "r_squared = regression.score(X_train_nor, y_train)\n",
    "adj_r_squared = r_squared - (1 - r_squared) * (X_train.shape[1] / (X_train.shape[0] - X_train.shape[1] - 1))\n",
    "\n",
    "print('Each coefficient in the above regression model:', regression.coef_)\n",
    "print('The intercept in the above regression model:', regression.intercept_ )\n",
    "print('The R-square of the above regression model:', r_squared)\n",
    "print('The adjusted R-square of the above regression model:', adj_r_squared)\n",
    "print('The p-value of each coefficient in the above regression model:', f_regression(X_train, y_train)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"anArray.shape\" function could be used to check the position of an array, we could use \".shape(0)\" to check the number of rows, and we could use \".shape(1)\" to check the number of column.\n",
    "* There's no any function to compute the adjusted r-square in the module \"sklearn\", so we need to calculate it manually.\n",
    "* \"f_regression()\" function could be used to check p-value in order to examine the significance of each variables. (If p-value is below 0.05 under the 95% of level of confidence, it indicates that the variable is significant.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Observation of Distribution of The Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = regression.predict(X_test_nor)    # Use the linear model to predict the price.\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "plot.style.use('ggplot')  \n",
    "plot.scatter(prediction, y_test)\n",
    "plot.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=1)    # Draw the dash line\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the scatters are closed to that dash line in the plot above, it indicates that the performance of our prediction is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate The Ridge Linear Regression Model\n",
    "\n",
    "* Ridge linear model was used to solve the problem of Multicollinearity, the difference between normal linear regression and ridge linear regression is the final regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each coefficient in the above ridge regression model: [[-1.17729906e-01  4.54993163e-02 -9.58510635e-03  2.49661353e+00\n",
      "  -1.09704320e+01  3.88166706e+00 -1.51932794e-02 -1.43149962e+00\n",
      "   2.25343243e-01 -1.14262661e-02 -9.61308497e-01  7.25133363e-03\n",
      "  -4.92443229e-01]]\n",
      "The intercept in the above ridge regression model: [34.34519137]\n",
      "The R-square of the above regression model: 0.7634862329498298\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl8U1X++P/XTVvaUpZSUyllVxyUEQVH3JiPMq6MG86MHFRG9COLyjIIgiIqIm4ossP4EdGPMD90PD8dB3UchUEH5aMjjAuCbCJlkJbSQilLaUub3O8fSWtpb5KbNGnS5P18PHi0SW6Sc5Lyzsn7vs85hmmaCCGEaP4c0W6AEEKI8JCALoQQcUICuhBCxAkJ6EIIESckoAshRJyQgC6EEHFCAroQQsQJCehCCBEnJKALIUScSG7i55NpqUIIERoj0AFNHdApKCiwfazT6eTAgQMRbE1sSsR+J2KfITH7LX0OXm5urq3jJOUihBBxQgK6EELECQnoQggRJySgCyFEnJCALoQQccJWlYtSajdwFHAB1Vrr85VSWcAbQDdgN6C01oci00whgucuLoSVKzBLSzAys2DQUBzZOQFvE+FT/3U2f3k1hzesxbV/X5O97on0XgdTtvgrrXXdupspwBqt9Uyl1BTv5QfD2johQuQuLsScOw2KCwHvBIhd23FPmOG57OO2eP2PHg2W78GGdVS4XT9djvDr7u/vIB7f68akXAYBy7y/LwNuanxzhAiTlStq/xPX8o7U/N4mwsfqdfYG81qRft1j4L0+ceJEkz2X3RG6CaxSSpnAi1rrJUB7rfU+AK31PqXUqVZ3VEqNAkZ5j8PpdNpvXHJyUMfHi0Tsd7j7XFJ2lCqr5yk7CuDztqwmft3j+b329R7UF8nX3d/fQaTf68rKSh599FHWr1/Pxx9/3CTvs92A3l9rXeAN2quVUtvsPoE3+C/xXjSDmS2ViDPKIDH7He4+uzNaW15f7eP6mtua+nWP5/fa13tQXyRfd39/B5F83X/44QdGjx7N5s2bSUpK4tNPP6VXr14hP15YZ4pqrQu8P4uAt4ELgP1KqQ4A3p9FIbVUiEgYNBTq50izczzX+7tNhI/V6+xIOvlypF/3Jn6vTdPkjTfeYODAgWzevJkuXbrw17/+lUsvvTQiz1dfwBG6UioDcGitj3p/vxqYAbwD3AHM9P5cGcmGChEMR3aO5wSoryoXP7eJ8LB6D8xfXk3ahrVUNFGVS6C/g3A6cuQIU6ZMYeVKTyi86aabeOaZZ2jTpk3Yn8sXwzT9L4ColDoNz6gcPB8Ar2mtn1JKnQJooAuwBxistS4J8HymLM4VWCL2OxH7DInZ73jt87Jly5g6dSotW7bkqaeeYvDgwRiGZ4HEMC3O1fjVFrXWu4BzLa4/CFwRSuOEEM1DItVwN9btt9/O7t27+f3vf8/pp58elTbITFEhhKWaGm7zi7WwfRPmF2sx507zBHnBvn37GD58OPn5+QA4HA4ee+yxqAVzkIAuhPAlBmq4Y9WHH37IlVdeyQcffMD06dOj3ZxaTb7BhRCieTBLrU+J+bo+EZSXl/PEE0+wbJlnTuWvfvUrnn766Si36icS0IUQlozMLMs9I43MrCZvSyzYvn07o0ePZtu2baSkpPDQQw8xcuRIHI7YSXRIQBdCWBs0FHZtPzntkqD1+qWlpdx4440cO3aM7t2788ILL9C7d+9oN6sBCehCCEtNWcMd6zIzMxk3bhy7du3iiSeeICMjI9pNsiQBXQjhkyM7B0bcH+1mRMW//vUvjh8/zuWXXw7AmDFjauvKY5UEdCGEqKO6upp58+Yxf/582rRpw5o1a8jJyYn5YA4S0IUQotbevXsZO3YsGzZswDAMhg0bximnnBLtZtkmAV0IIYB3332XBx54gCNHjpCTk8OCBQvo379/tJsVFAnoQoiEN2vWLObNmwfA1VdfzezZs8nKan7lmbFTQCmEEFEyYMCA2kW1XnnllWYZzEFG6EKIBGSaJv/617+4+OKLAejXrx/r16+nXbt2UW5Z48gIXQiRUA4ePMgdd9zBzTffzD/+8Y/a65t7MAcZoQshQtQcl9b95JNPGD9+PEVFRWRmZuJ2u6PdpLCSgC6ECFrN0ro1ywKYALu2454wIyaDelVVFbNmzeKPf/wjpmly0UUXsWDBAjp27BjtpoWVpFyEEMFrRkvr5ufn85vf/IbFixdjGAaTJk1Cax13wRxkhC6ECEFzWlq3ZcuWFBYW0rFjRxYvXky/fv2i3aSIkYAuhAharC+te+zYMVJSUkhNTaVdu3YsX76c3NxcMjMzo920iJKUixAieIOGepbSrStGltbduHEj11xzDc8++2ztdb169Yr7YA4yQhdChCAWl9Z1u928+OKLzJw5k+rqatatW0dFRQVpaWlRa1NTk4AuhAhJLC2tW1RUxPjx4/nkk08AGD58OFOnTk2oYA4S0IUQzdyaNWuYMGECBw8eJCsrizlz5nDVVVdFu1lRIQFdiCZWd0LO4fYdcA+8OSZrt5uLFStWcPDgQX75y1+yYMEC2rdvH+0mRY0EdCGaUP0JORXbN8HWb2N2Qk6sMk2zdsOJ559/nksuuYS77rorpjZsjobE7r0QTa0ZTciJRaZp8vrrrzNkyBCqqqoAyMrKYsSIEQkfzEFG6EI0qeY0ISfWHD58mAcffJB3330XgPfff59BgwZFuVWxRT7ShGhCvibexMqEnFi1YcMGrr76at59910yMjKYP3++BHMLEtCFaEoxPCEnFrlcLubNm8fvfvc79u7dy7nnnsuHH37IzTffHO2mxSRJuQjRhOpPyElr34FKqXLx6b333mPWrFkAjB49msmTJ9OiRYsotyp2SUAXoonVnZDT1unkwIEDjX7M5rg2uR033ngj//znP7npppu47LLLot2cmCcBXYhmrrmtTe5PeXk5zz77LMOHD6dz584YhsHcuXOj3axmQ3LoQjR3cVIKuW3bNq677jpeeuklJkyYgGlareco/JGALkQz19xLIU3TZNmyZVx33XVs376d0047jenTp9dOHBL2ScpFiGYu1tcm96ekpITJkyfzwQcfAHDLLbcwY8YMMjIyotyy5sl2QFdKJQH/BvK11tcrpboDfwaygK+A27XWJyLTTCGET4OGwq7tJ6ddmkEpZEVFBddddx179uyhdevWPPvss1Jb3kjBpFzGA1vrXH4WmKu1PgM4BAwPZ8OEEPY4snMwJszAuPAy6Nkb48LLMJrBCdG0tDSGDRvGL37xC1avXi3BPAxsjdCVUp2A64CngIlKKQO4HLjNe8gyYDrwQgTaKIQIIJbWJvfnxx9/ZNOmTfTu3RuAu+++m5EjR5KcLNnfcLA7Qp8HPAC4vZdPAUq11tXey3uB+NtCWwgRNitXruTqq6/mlltuIT8/HwCHwyHBPIwCvpJKqeuBIq31l0qpAd6rrU4/W9YYKaVGAaMAtNY4nU77jUtODur4eJGI/U7EPkNi9LusrIyJEyfy6quvAp7JQp06deKUU06JbsOaUFO9z3Y+GvsDNyqlrgXSgDZ4RuyZSqlk7yi9E1BgdWet9RJgifeiGcysOGeYZtE1N4nY70TsM8R/vzdv3sy9997Lrl27SEtLY9q0aUycOJGDBw/Gdb/ra+z7nJuba+u4gCkXrfVDWutOWutuwC3AR1rrocDHQM0KOXcAK0NrqhAiHr3xxhvccMMN7Nq1i549e/K3v/2NO+64Q+rLI6gxyasHgT8rpZ4EvgZeDk+ThBBNJZJrwPTo0QO3282wYcOYNm0a6enpYXlc4ZvRxNNrzYICy8yMpXj/OupLIvY7EfsM0e13/TVgAPCWQIYa1Hfu3EmPHj1qL+fl5dG9e/eTjknE9zpMKZeAX21k6r8QiSqMa8CcOHGCJ598kgEDBrBq1ara6+sHcxFZUi8kRIIK1xoweXl5jBkzho0bN5KUlMSePXvC0TwRAgnoQiSocKwB8+abbzJ16lTKysro1KkTixYtol+/fuFrpAiKpFyESFSN2A7v6NGjjBs3jvHjx1NWVsaNN97IqlWrJJhHmYzQhWimGluhUn87vGAew+Vy8cUXX5Cens6TTz7JkCFDpBwxBkhAF6IZCtcuRcGsAeN2u3G5XKSkpJCZmcmSJUto1arVSVUtIrok5SJEc9TEuxTt37+f2267jaeeeqr2uj59+kgwjzES0IVohppyl6LVq1dz5ZVX8umnn/L2229z6NChsD+HCA8J6EI0Q74qUcK5S1FFRQXTpk3jzjvvpKSkhEsvvZTVq1fTrl27sD2HCC/JoQvRCJGcOu9XhHcp+v777xk9ejRbtmwhOTmZhx56iFGjRuFwyBgwlklAFyJE4ToxGYrGVKjYMW/ePLZs2UK3bt1YvHgxffr0CcvjisiSgC5EqPydmGyC3YMiuUvRE088gdPpZPLkybRq1SoizyHCT74/CRGiSJyYdBcX4l46G9fzD+NeOtuT0mkCGzZsYOTIkZw44dnnPSsri8cff1yCeTMjI3QhQhSOqfN1RSOF43K5WLBgAXPmzMHtdrN8+XJGjBgRkecSkScjdCFC1Yip85aauLY8Pz8fpRTPP/88brebMWPGMGzYsIg8l2gaMkIXIkThPjHZlLXl77//PpMnT6a0tJRTTz2V+fPnc+mll4b9eUTTkoAuRCOE88RkuFM4vqxbt46RI0cCcMUVVzB37tyE2rA5nklAFyJWRLi2vEb//v259tprueiii7jrrrtkUa04IgFdiBgRqdpy0zRZvnw5AwYMoGvXrhiGwZIlSySQxyEJ6ELEkHDXlpeUlHD//fezatUq+vbty8qVK0lKSpJgHqckoAsRpz777DPGjRtHYWEhbdu25Z577iEpKSnazRIRJAFdiDhTVVXFnDlzWLhwIaZp0q9fPxYtWkSnTp2AKK4/IyJOAroQzVj94Oy+4VZuHfMHPv/8cxwOB/fddx/33XcfycnJtcc3mLz05We4ft4XY8iI2sAuQb95koAuRAwJJpBaBWdj13auvOQidu/ezaJFi7joootOvpPV5KXqKti4HrNgj+ekLERt0THROBLQhYgRQU/99wbnsmoXO45V0DczA4oLGdHtZ9y6Zg1t27ZtcBe/k5TqzkqN4qJjInQy9V+IWBHk1H+ztIRvD5dx7bqt3L7+e/aWVwJgHDlkGcwh8CQls7SkSWesivCSgC5EjAgmkLrdbl7aksdvPttO3vFKctNbUOnyzDP1G7St1p+pw8jMapLdkERkSEAXIkbYDaTFxcUMGzaMJz5cS5VpckfXbN655ExOb5UWcGapIzsHY8IMOPcCSEk5+caa+4Z70THRZCSHLkSssDH1/7PPPmP06NEUFxeTmZnJ8489yjVH8oOqRnFk58DYR/yegI3kbkgiciSgCxEj7Ez9b9myJYcOHeLiiy9m4cKFdOjQoVHP5+skZyR3QxKRIwFdiBhSE0hrR8/LFlKcksYpt43EkZ1Dnz59eOutt+jbt6/M+hQNSEAXIgIaMzGnpnzRLNrHm/klTPtuD3PXb+DX85bgyM7h/PPPj3DrRXMlJ0WFCLPagPzFWti+CfOLtZhzp9nfH3TlCo4W5DN+427u/3Y3ZS43n+7aE7Gdi0T8kIAuRLg1ciu5L7duY+C6Lfy1oISWSQ5mn9OVJ3/eWerARUCSchEizIKdmFOTnqkuOcD/bN7F8x/9Hy7T5Ow2LVnUpzuntUoDpA5cBCYBXYgwC2YruerCgtrp/serXLz22RZcpsmos7oxuUs7UpO8X6KlDlzYEDCgK6XSgE+AVO/xb2qtH1NKdQf+DGQBXwG3a61PRLKxQjQLQWwlV/b6EsyifRiGQeuUJBb26c6RahcDrrwKIy1d6sBFUOyM0CuBy7XWx5RSKcA6pdTfgYnAXK31n5VS/wMMB16IYFuFaBbsbiVXUVHBo3olZskBHu/VGYDz2rXy3liOY+wjfp9HlrgV9QUM6FprEzjmvZji/WcClwO3ea9fBkxHAroQQOCJOTt27GD06NFs3bqVFg6DEd1OpXPL1NrbA+XLg16ZUSQEWzl0pVQS8CXQA1gM/ACUaq2rvYfsBTr6uO8oYBSA1hqn02m/ccnJQR0fLxKx34nSZ9M0efnll5k0aRLl5eWc3q0bi3p3obOr7KeD0tJxlBwg5U+LyLh1FMk5uQ0e5/CfFlFhUUmT+sGbtJ0wPbKdaKREea/raqo+2wroWmsX0EcplQm8DZxlcZjVeSC01kuAJTXHHDhwwHbjnE4nwRwfLxKx34nQ50OHDvHAAw/w/vvvAzB48GAWTp/Gif/vj56cu9sNVSegohzX99/h+v47KrZ+i2Ex6nbt32f5HBX791EV469jIrzX9TW2z7m5DT/UrQRVh661LgX+CVwEZCqlaj4QOgEFwTyWEInmueee4/3336dVq1YsWrSIOQ9PwfX8w7BxPRw9DGVH4UTlyXeqV7/uLi7EvXQ2FOyxfA4pbUxsdqpcsoEqrXWpUioduBJ4FvgYuBlPpcsdwMpINlSIcGvqk4oPPvggBw4c4JFHHqFr1664l87GtT8/4P1q6tfr580bkNLGhGdnhN4B+Fgp9S2wAVittX4PeBCYqJTaCZwCvBy5ZgoRXo2enm9Dfn4+U6ZMobLSM+rOzMzkpZdeomvXroD9HYBqR91WM1ABWrfFuPAyy9SMSCx2qly+BfpaXL8LuCASjRIi4vxNzw/DsrF/+9vfmDx5MocPH8bpdDJp0qQGx/iagHSSOqNunx8AuV1wyFK3ApkpKhKUz+n53/7bk6MOMf1SXl7OY489xooVnrz3lVdeyV133WV98KChJO3eeXLaJSsbOneHivIGaaBgZqCKxCQBXTSJWJsE43N0XF7mScOEUNP93XffMWbMGL7//ntSU1N59NFHufPOOzEMw/J4R3YOmdPnU/LqQnuvSxAzUEVikoAuIs5yEsxXn+Hq1RdjyIgmC+x1P1RIS/eMhkuKrQ8OMv2ydetWbrjhBiorKznjjDP44x//SK9evQLeLzkn13a6xO4MVJG4JKCLyLPKV1dVwcb1mAV7PEEqwpMuLCtE2jk9myXv+A7KyxrcJ5jlas8880wGDBiA0+nk8ccfJz09PRzNbkC2hhP+yHroIuL8BsbiQszZj1BdGOFpDFYfKocOYKSlY5xjvQNQoNz0unXryMvL8xxrGLz44os899xzEQvmQgQiAV1EXMCTdgeLKJ0+Pqwlg/X5XaN80FBPLrouP7npqqoqnnnmGW655RbGjh3LiROeRUZTUlLC2mYhgiUpFxF5Vifz6nHtz8cIU8mgFX8VIsHkpv/zn/8wZswYvv76axwOB1dccQUOR9OPi2LtJLOIDRLQRcTVBEzzjaXw3ddQXWV5XES3WAtQIWInN/32228zZcoUjh07Rm5uLosXL+aCCwJPxQh38JWVFoUvEtBFk3Bk58DYRzzBaPYjcLCowTGRqqeuCai0autZAKtNJsapHYIKrA888EBtbfm1117LrFmzyMzMtPXcYQ++EZ4UJZovyaGLJuXIzsG4/8kGOeuk9h0jUk990hT/vO2eD5JjR4IeJf/sZz8jLS2N5557jiVLltgK5kCjN4y2EuyepSJxSEAXTc6RnYMxYQbGhZdBz94YF15G5vT5kUkXhBhQ3W43O3furL08fPhwPv74Y4YOHepzopCVSARfX99kZMaokJSLiIr6OetkpxMisEZ2KAG1qKiI++67j6+++opVq1bRpUsXDMOgS5cuQT9/RKbry4xR4YOM0EVcC3Y0+9FHH3HVVVexdu1akpOTyc8PvLytX0GWRNph9Q1HVloUICN0Ee9sjmYrKyuZOXMmS5Z4Ntfq378/CxYsICencUEyUtP1ZcaosCIBXcQ1OwH1hx9+YPTo0WzevJmkpCQeeOAB7r33XpKSkgI+vp2SRDvBV+rKRThIQBdxzyqg1g2gR49XsX3bNrp06cLixYs577zzbD1uuEoSpa5chIsEdBGzGjNqdRcXeiYy7druueK0nrUrO7qLCyl//mHSvCst9gZeuqwvFzz+PG2797DfwHDVg0tduQgTCegiqmqCdknZUdwZrWuDdmNGre7iQsznHz55adyN6zHzvsfV/Qy+Wr+ecZ9tYuqZnbiuQzsALk91Y6xZGVQADVdJotSVi3CRgC6ipm7Qrl0MwBu0GzVqXbnCcp1z1+ESXnhrJbO/L8Blwoo9xVybk1lbV25u+aZ2gTBfo/u6wlWSKDsRiXCRgC6ix0/Qbsyo1eqYwooTjP8mj89LjgFwd/f2TO6Ze/IkoaOHMWdN9aw1c/TwT9dvXI+5ZxfuyU+fHNTDVQ8udeUiTCSgi6jxF7QbM2qtf99V+0uZ9O1uSqtcZLdIZs653bgsu631nQ/5mNx06ECDbweBKmiCOgeQ2wUqyj2/+/hGIEQgEtBF1PgN2o0ZtQ4aCt9vgZJiKl1uZmz9kdIqFwOy2zD7nG5kp4a2brnVB1D9Chp3cSHupbMxiwqh4D9QWeG5L1ieA6guLGi4k1LBnpDaJ4QE9BjjLi7k8J8W4dq/LybrkX2NOkOqSBk01LP9W91RcTvnT48ZoH7c13M6snNwT3oKc/liUnduYeF5Pfj3sRMM79kFR91USpACfTuw3OauruJCzGcm4+7Vp7atZa8vkQoXETYS0GNITUCoiNF6ZF+VJ65h42D5wtDqqOsvdFXnsr8JOb7aUn37WFbMe57de/N5pPspUF1F3zbp9G2TDiktoG0WHA6xeiTAtwPzjaV+N/EAPHn6L9bWvj6uEusUj1S4iFDIWi6xJAJLrYaVr/a9Oj+0dltVo5QU2+uvRVtK8/dy9+9vZep7a1jyzTY2FdULiiXF0K1Hw7VVDAMyWns2jO7U3fr5Onc/KTfuXjob1/MP414623O5uBC2fB243TW8r09SlvXm2FLhIkIhI/QYEuv1yD7bcbwsuOMD3B5KJcsXJUf5wzd57KuoonWyg2fO7krvti0b3rGiHKP+7kmmCWVHPbnr3C6wN6/B3Yxcz0qLvr4ZkNsFqqx3YvLXh4z7HqNi67dS4SLCQgJ6DIn1emRf7aNlBpQ3DOqB2h2OSpZqt8n8nftYuHMfbuC8zAwW9OlOl5apPu/nyM7BnZaOWX8rvOJCT2DOzvEdYH19S6mpUGnwhAakZ8DxY5ZtSc7JxYjA4l0iMUlAjyVRqke2fULTV/vq5dBtt7uxlSy7trPws6+Yv3MfBjDu9BzuOyOXFIePDShS0zCLaqpQ9lkf4x3B+3o9gv62dE4/jCEjGp4sDXI/UyHsMEzTcswVKWZBQYHtg51OJwcisOlBLHMXF5L6wZtUNFGVi2Vlhne9bavnDWuVS53HSy47SnWdqf/+nhfDgMJ8KD/O4coT3PnlD0w+53QuSXE1fAKHwzPqLi6sLSEEIDXt5MtexoWX4fATXF2LnoSN6xvecOY5nu3tfLyOvl6fRPwblz4HLzc3FyDgVlkS0GNQU/bbvXS2p+qinkCBLdz89bnuh87xahdL8vZzz2k5pCV5zumbhgNj+ISG6ZDUNBj7KMa6VZZ9xOHwbBpdw88HWQ2fAf3cCzCGjAj6Qy0R/8alz8GzG9Al5ZLgYv1ELIC5fBEUF7LlyHHGfp3HzrIKjla7ePSszgAYphve/pNn82mLgOr6y3LrB64bzFNawLBxgb9V+MqVV5RL6kREnZQtJrhY33DYtW0T5taNvLK7iBs/28bOsgrOaJXG4I71yv2Ol3kC6qChnhOmpSWwcgWubZs8MzYDqToB/1gZ8LBYf71EYpMReqKL8YWhDv7Pc9z/7x/4qNgzw/P3XZw8elZn0pPqjUVaZliXFH7zhWWu3FLN6or+xPjrJRKbBPQEF6k9L0NV9+RhoZHCde98SnFlFW1Tkniud1d+ndPO+o7ZHaxnatoN5jbF2uslRF0S0EWT5n5PqlZJS/dcWVHO4fYdcPW7DF6ZW7u2S3vT5BeZGRyqqmb+ud3JTW/h+4G3bYSU0BbdqnVaT1uHSa5cxKqAAV0p1RlYDuQAbmCJ1nq+UioLeAPoBuwGlNb6UOSaKmJdoNJFf4tXVWzfBP9ay+5DRwDolpGKYRjMPbcbaUkOkuqv+WLF7kzNdk7PCdG6a7pkZXuqVIRoxuycFK0G7tdanwVcBIxRSvUCpgBrtNZnAGu8l0WCcm3bhPn4Hzzlgds3YX6xFnPutNodgCDw4lV/ySvg1+u2MObrXZzwVqBkJCdZB/PkEEbjhuEpL5z8NMaDMzEuvAx69sa48DKMSU9J2kQ0ewFH6FrrfcA+7+9HlVJbgY7AIGCA97BlwD+BByPSShHT3MWFsOiJhvnqOsvAuosLPWunWDhW7eKR7/bwl3zPiLlzyxaccJu0qDvc6NwdI7fLT5tfHC71pFmCYZoYaek/BW5Jm4g4E1QOXSnVDegLfAG09wZ7tNb7lFKnhr95Ita5iwsxn3vI58nH2n06V67wLIRVz8bSMsZ+k8d/jleS5jB4/OdduKXTKSdvDQeeskRvAHYXF8LMB0JqbyzV1wsRbrYDulKqFfAWcJ/W+ohSyu79RgGjALTWOJ3Wy4VaNi45Oajj40Vz6Xd1YQGH5jyKWXrQ90FHD+OY/zi0yaT+xPyleft5etteqk3o1Tqdxb/qx+nV1is3Gq3a1L4mh/+0iIojpSG1Oa19B9rG0GvbXN7rcJI+R/B57ByklErBE8xXaK3/4r16v1Kqg3d03gEosrqv1noJsMR70Qxm+msiThGG5tNv96sLMQ/sD3ica3++5eg82TCoNuG/+5zF47+7HuOGWzyjfYsPCPPYkdrXxLXfx8Ja9VlM7a/odxmVMx+KmZLD5vJeh5P0OXjeqf8B2alyMYCXga1a6zl1bnoHuAOY6f0ZeJqdaBbsLrQVVPqibTtwODiw90ec3j097zi/N+dOeYLzr7gKp9NJ0dbNnhmbvu7v5XMZ37q8q0Aa61b9lHf/5dWwfCFmjO4IJURj2Rmh9wduBzYppb7xXjcVTyDXSqnhwB5gcGSaKEIVygqIvjZwqB/03MWFYGN0XuNEOyfPHAK9bjV/u+16unfrhjFoKOd7H7Ni09eYT93veyKQd5cgBg31/Pv3OnBZrK6YmobR58LmCjlZAAASY0lEQVSf+npm75/avHR2bTCv+7iyf6eIF3aqXNbhe5WvK8LbHBEudgNzA/62watzUtKcO82zXKwNO1u0Ypz+kO+2byc5OYlvjpTTrbQEY+UK3N4p80eenuR/VmedvTiNCTPg7F9Yrnpo9LnQ5yqRzWEhMiEaQ2aKxisbgdmKraBn9dh1nXkuRttM3IcOoncVMO2DTymvqKBLp44sOqcbfcqLYHvRydu3+VrFsD5vH4whIzAL9gS1pkqs7wglRGPJaotxKtTRqJ3VBAOOaE03RwePYOwPJUx+ZzXlFRX89re/5YPbb6IPFrXqdhbFqvvwpSU4vGuXnzQ5KNC3j0FDG24QLQtriTgiI/Q4FfJo1MZqgoFOShqZWfz44498+OGHZGRk8PTTT3PzzTfjev7hoPrg7/Eh+DVVZGEtEe8koMerEJd5tRX0rB4bcJsmjlM7wKChnJ2dw7x58zjnnHPo3r074OeD4LSeJBXme8obA/HTBzsngWVhLRHPJKDHKUd2Dq5h4+DV+XC8DFpm+N2RxyoYJvkbudbkvV0uaJHKvhYtGf/PfzNswGAGee83aNCgk+/j40PGGDKCzHZZlLy6sMEqjHV/9zeiDvkksBBxRAJ6nHIXF8LyhT9VopSXwfKFlgEumGBotWLihwePMfmrLyg9fJgDL73M9bcOJSkpqUGb/H3IJDudOGrWfAklJRLiSWAh4omcFI1X/gJcmI6tcLl5ePMeRn78JaWHD3P55Zfz1ltvWQZzqPchU17m+bnoCc82cfz0YeFvxUZfzCLrYxrUnQsRx2SE3oz5G80GU+Xi89iihlPsa47dfrScMV/vYsexClo4DB76r36MXL684aJadVl9cFRWwKInqO7RM6RRdu3CXz/usn7Ow7JEv0gcEtCbqUBpkmCqXHyerCzYg7u48KSUh5GZhcs0GfdNHjuOVXBaRiqL+pxG78t/6T+Y46fcsbKCsteXBF1q6W/DjFptMv22SYh4IimX5ipQmiSYmutBQyE1reH1lRUN0y6DhuI4tQOzenfl1s5O/tb/LM7ucVrA6plASwW4Sg7YqoE/SaAJToBxage/twsRT2SE3kwFGs0GU3PtyM7BldsV8hpO8Kl5vM8//5x169YxefJk3BNm0GflCs61eeLSzlIBSVlOqgfeHFSpZcAJTjJpSCQYCejNlJ2USk3NdW2ufdlC3D4CsHFqDqZFQHe1zmT2c8+xYMECTNPkwgsv5NJLLw2uciTQSDo7h4xbR1GV3MLyQwi8C2vV+wDxmSpq3RajVx+ZNCQSjgT05srmxCHbJYkWj/djehv+8NeP+PLbbzEMg/Hjx3PJJZcE3VSfI+n0DIxzzodBQ0nOyYUDBxpM/PHXfp917VJ7LhKUBPRmynZKxWblSP3He3dvMQ/9fS1Hjh0jJyeHhQsXhhTM/eXOjXPO97kyop32O0bcL1P5hahDAnozZmcau7+SRKs0BiPu57XXXmPy3MkADBw4kFmzZpGV1fDEZKBJQH5z56lpng0nArBzrkAmDgnhIQE9zvkrSTTzdgAN0zDXX389L730Ev/93//N7bffblmOaCuV4y93Xlnhc+aqnfbLkrdCNCQBPd5Z5ZlT007aTMI0Td78ejM3vrmMlvc+SJs2bVi9ejXJyX7+PHykQsw3luJOS/fM3PQ12afO8QGn5oe4yJgQiUgCepyzyrWbRYW1JYoHKquY9O1uPio+wibHKp6690EA/8EcPyc6t3yNWdVwQ+igH8dP+yVPLoQ1CegJoEHlyNLZmHnb+aT4CBM25lF8oprMlCT6n/mzBvf1lSf3mcoJIpiDvdSJ5MmFsCfmA3rIq+8Jn05cq3hO/5UXv/OkRC7KasX8yy8gd8pjQJ3XvGgfFOypTc8ELBlMToFqPwHdMMCs8zEgqRMhwiqmA3p1YYGscR1mpbu+59YhQ/i2YD9JhsGEC85m7MArSPrN7Z70RqD1UfyUDJoV5ZYbN9c6px9GWrp8OAsRITEd0MteXyJrXDdS3W84pKXTas8uOrkrKUlvwYI+3Tn/Z10wvMEcwHxjacD1UXyVDLqLCxtu3FzDu5GFBHAhIiemA7qr5IDl9QHX8BDAT6WFRwvyOVxdTaf0VAzg2d5dAWibklz7AekeNNQTzP2NsL185b1POoFZXOhZurZNpmeBrDqjcas0Gk5n2PotRKKK6YCelOXEKiMrNcg2rVzBN9//wNhvdtE6OYm/XnwmqUkOTyCvwyzaB4GWoa0RIO8d6ASmr/r16hmLILmFnV4JIXyI6eVzM24dZX8JWHESt9vN4tVr+e3n29hz/AQAh6qqrQ8+Uho4mGe0xrjwssavk+Kjfr3s9SWhP6YQAojxEXpyTi6G1CAHbf/+/YwfP55P130JwPBupzKlZ0dSkyw+v7NzoFVbv0vbAtCpW+B1V2zwlS7zlV4TQtgX0wEdpAbZrpq89Jqvv2Xiqv+j5Hg5p7Rrx+zzenB5qvunA9s5octpUFH+U/565QrLpXPrCleay1f9elKWE7fF9UII+2I+oIvA6ual8/9TTMnxcv6rg5N5f1rBqU5nwG84bqua8rrCmebyMZU/49ZRlIbnGYRIWBLQY0Tdyo/D7TvgHniz7dRS+ZuvkuYNkL/v4sSZmsw17TNJ+r8PPWmSAN9w6k+vJy3dc0OdUXy40ly+pvLXrIdul0w4E6IhCegxoH7lR8X2TbD124ATqEzT5PXXX+e5OS/xVr/T6ZaRhmEY/Dqnnef2IMo7mzK11djnsr1phxAJJqarXBJGoA2fLRw+fJh77rmHyZMnU3y8gnf3HWpwTNyWd4bwegmRCGSEHgMCbeJQ34YNGxgzZgz5+flkZGTw9NQp/GbHvxJmidlgXy8hEoUE9BhgdxMHl8vFggULmDNnDm63mz59+rB48WK6deuGu/jaoHLKzTkHLZteCGFNAnossLmJQ15eHgsWLMA0TcaMGcOkSZNo0cIzuzKYvHSzz0HLphdCWJKA7oedPTPDMcqtX/mR1r4DlRZVLj169GDmzJl06NCBSy+9NPSO2dw4OlbJphdCWJOA7kOgUWy4R7l1R9htnU4OHDhAeXk506dP5+KLL+amm24CYMiQIY3uW6g56FhK08iEMyEaChjQlVKvANcDRVrrs73XZQFvAN2A3YDSWjcss2jOAo1iIzzK3bJlC2PGjGHHjh38/e9/55prriE9Pb3Rjwuh5aCbfZpGiARgp2zxVWBgveumAGu01mcAa7yX40qgUWykKi1M0+SFF17g+uuvZ8eOHfTo0YPXXnstbMEc8OSag130TEoFhYh5AUfoWutPlFLd6l09CBjg/X0Z8E/gwXA2LNoCjWIbW2lhlb4oTWrB/fffz6pVqwC47bbbePzxx2nZsmWIvbAWSg5aSgWFiH2h5tDba633AWit9ymlTvV1oFJqFDDKeyzOIDYySE5ODur4cKq+cxylu3fi2p9fe11S+45k3jmOZKcz4O1+H7uwgNL5j9fe1wSSdu9kzLZiPvn8czIzM3nhhRf47W9/G5G+AZ4NJc56xvbhh9t38MxgrSetfQfahuE9iuZ7HU2J2G/pcwSfJ9JPoLVeAtQsdm0eCGK9Dqf35GBUJLfAPf4xjDqjWPegoZQmt/CsORLodj/cry7ErPNBAODan8+D55yJy+Hgf//3f8nIyIhe3y24B94MW79tUCpYOfDmsLQzqu91FCViv6XPwcvNzbV1XKgBfb9SqoN3dN4BCLCYdvMUqJIi1EqLmjTFnuOVfFBYyqjT2gPQu2VK7beYWPuDl1JBIWJfqAH9HeAOYKb358qwtSiGRKpMz8jM4q8FJUzd/B+OVrvp0rIFA3PaxfxMRykVFCK22SlbfB3PCVCnUmov8BieQK6VUsOBPcDgSDYyGtzFhZizpsIhz0jZBNjxHe7JTzcqqJeVlfHwl9/z/3+TB8DA9plclNU66jMdY6nGXAgRGjtVLrf6uOmKMLclpphvLK0N5rUOHfBcP/aRkB5z06ZN3HvvveTl5ZGamsr0q/6L23p0xNHulJACaLiCsNSYCxEfZKaoL7t8bMnm6/oAPvroI+666y6qqqo466yzWLx4MT179gy5eWENws18KQAhhIesh95E+vXrR8eOHbnzzjt59913GxXMgbBO9JEacyHig4zQfTmtJ2xcb329TZ999hl9+/YlPT2d1q1b88EHH9C6deuwNC/YIOwvPSPL0QoRH2SEbsFdM/JNTjn5hqxsjCEjAt7/xIkTzJgxg8GDBzNjxoza68MVzMF3sLW6viY9Y36xFrZvwvxiLebcaT/1M5SlAIQQMUdG6PXUz00DnsD+874YQ0YEzE//8O/1jLn3HjYV7CfJYdChbWtM08QwjIDPG9Qm0cGsCR4gRy415kLEBwno9VkFv+oqjLT0gBs261eW8siMJzhe7aJzegsW9unOeUd2Yx7YjxFg96BgN4kOJgjbSc9IjbkQzZ8E9HpCOUF44sQJJk6cyNtvvw3ADR3a8czZXWmTkmSvWiTEKhO7QVhy5EIkBgno9YQS/FJSUnC5XKQnJ/PEWR0Z3OmUk1IsgapFIl5lIlu2CZEQJKDXZzP4ud1uSkpKcDqdGIbBzJkzmdg1i9N+aLgiYaCRcKRH0JIjFyIxSECvx07wKyws5A9/+AMlJSW89957pKWl0bZtW1oPH9fwhKqdkXATjKAlRy5E/JOAbsFf8Fu1ahUTJ07k0KFDOJ1Odu/ezZlnnll7v1BGwnY3iRZCCH8koNtUUVHBU089xSuvvALAgAEDmDdvHtnZ2ScdF+pI2GqT6Lpk8SwhRCAS0G3YsWMHo0ePZuvWraSkpPDQQw8xcuRIHI6mmZcli2cJIeyQmaI2rF+/nq1bt9K9e3feeecd7r777iYL5oBs0CyEsEVG6DYMHTqU6upqBg8eTEZGRpM/vyyeJYSwQwK6DYZhcOedd0bv+WVikBDCBkm5NAeyeJYQwgYZoTcDMjFICGGHBPRmQiYGCSECkZSLEELECQnoQggRJySgCyFEnJCALoQQcUICuhBCxAnDNK2mrERMkz6ZEELEEf8bE9P0I3QjmH9KqS+DvU88/EvEfidinxO139LnkP8FJCkXIYSIExLQhRAiTsR6QF8S7QZESSL2OxH7DInZb+lzhDT1SVEhhBAREusjdCGEEDbF7OJcSqmBwHwgCViqtZ4Z5SZFhFLqFeB6oEhrfbb3uizgDaAbsBtQWutD0WpjuCmlOgPLgRzADSzRWs+P534rpdKAT4BUPP/v3tRaP6aU6g78GcgCvgJu11qfiF5Lw08plQT8G8jXWl+fIH3eDRwFXEC11vr8pvj7jskRuvcPYDHwa6AXcKtSqld0WxUxrwID6103BVijtT4DWOO9HE+qgfu11mcBFwFjvO9vPPe7Erhca30u0AcYqJS6CHgWmOvt8yFgeBTbGCnjga11LidCnwF+pbXuo7U+33s54n/fMRnQgQuAnVrrXd5P7j8Dg6LcpojQWn8C1N9LbhCwzPv7MuCmJm1UhGmt92mtv/L+fhTPf/aOxHG/tdam1vqY92KK958JXA686b0+rvoMoJTqBFwHLPVeNojzPvsR8b/vWA3oHYEf61ze670uUbTXWu8DT/ADTo1yeyJGKdUN6At8QZz3WymVpJT6BigCVgM/AKVa62rvIfH4dz4PeABPag3gFOK/z+D5sF6llPpSKTXKe13E/75jNaBbzYqScpw4o5RqBbwF3Ke1PhLt9kSa1tqlte4DdMLzLfQsi8Pi5u9cKVVzbujLOlcnyv/t/lrr8/CkjccopS5tiieN1YC+F+hc53InoCBKbYmG/UqpDgDen0VRbk/YKaVS8ATzFVrrv3ivjvt+A2itS4F/4jl/kKmUqilOiLe/8/7Ajd4ThH/Gk2qZR3z3GQCtdYH3ZxHwNp4P8Ij/fcdqQN8AnKGU6q6UagHcArwT5TY1pXeAO7y/3wGsjGJbws6bR30Z2Kq1nlPnprjtt1IqWymV6f09HbgSz7mDj4GbvYfFVZ+11g9prTtprbvh+T/8kdZ6KHHcZwClVIZSqnXN78DVwGaa4O87JssWtdbVSqmxwId4yhZf0Vp/F+VmRYRS6nVgAOBUSu0FHgNmAlopNRzYAwyOXgsjoj9wO7DJm1MGmEp897sDsMxbweUAtNb6PaXUFuDPSqknga/xfNDFuweJ7z63B95WSoEnxr6mtf5AKbWBCP99y0xRIYSIE7GachFCCBEkCehCCBEnJKALIUSckIAuhBBxQgK6EELECQnoQggRJySgCyFEnJCALoQQceL/ATri6kSTjn2aAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ridge=linear_model.Ridge(alpha = .5)    # Initialize the model\n",
    "model_ridge.fit(X_train,y_train)    # Train the model\n",
    "print('Each coefficient in the above ridge regression model:', model_ridge.coef_)\n",
    "print('The intercept in the above ridge regression model:', model_ridge.intercept_)\n",
    "print('The R-square of the above regression model:', model_ridge.score(X_train,y_train))\n",
    "\n",
    "prediction = regression.predict(X_test_nor)    # Prediction\n",
    "\n",
    "plot.scatter(prediction, y_test)\n",
    "plot.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],'k--',lw=2)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It seems that there's no problem of Multicollinearity in this case.\n",
    "* The tutorial website I refered to: https://blog.csdn.net/u013597931/article/details/79915804\n",
    "* To get more information about variable selection, I could visit the website at: https://taweihuang.hpd.io/2016/09/12/讀者提問：多元迴歸分析的變數選擇/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate The PCA and Decompose The Original Sample Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we select  0  variables,  0 % of the original variance was covered.\n",
      "If we select  1  variables,  46.0 % of the original variance was covered.\n",
      "If we select  2  variables,  57.0 % of the original variance was covered.\n",
      "If we select  3  variables,  67.0 % of the original variance was covered.\n",
      "If we select  4  variables,  74.0 % of the original variance was covered.\n",
      "If we select  5  variables,  81.0 % of the original variance was covered.\n",
      "If we select  6  variables,  86.0 % of the original variance was covered.\n",
      "If we select  7  variables,  90.0 % of the original variance was covered.\n",
      "If we select  8  variables,  93.0 % of the original variance was covered.\n",
      "If we select  9  variables,  95.0 % of the original variance was covered.\n",
      "If we select  10  variables,  97.0 % of the original variance was covered.\n",
      "If we select  11  variables,  98.0 % of the original variance was covered.\n",
      "If we select  12  variables,  100.0 % of the original variance was covered.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Check which dimension is the best\n",
    "for j in range(0, len(boston['feature_names'])):\n",
    "    pcaModel = PCA(n_components = j)    # \"n_components\" means the number of dimension we need to keep.\n",
    "    new_X_train_nor = pcaModel.fit_transform(X_train_nor)    # Decompose the model\n",
    "\n",
    "    varianceRatio = pcaModel.explained_variance_ratio_    # \".explained_variance_ratio\" could be used to examine the percentage of variance new model could covered.\n",
    "    aggregateRatio = 0\n",
    "    previousOne = 0\n",
    "    \n",
    "    for i in range(0, len(varianceRatio)):\n",
    "          aggregateRatio += varianceRatio[i]\n",
    "    \n",
    "    print(\"If we select \", j, \" variables, \", str(round(aggregateRatio * 100)), \"% of the original variance was covered.\") \n",
    "    \n",
    "# I think that 90% is enough.\n",
    "pcaGoodModel = PCA(n_components = 7)    # Initialize the model again\n",
    "pacGoodModel_fit = pcaGoodModel.fit(X_train_nor)    # Train the PCA model\n",
    "new_X_train_nor = pacGoodModel_fit.transform(X_train_nor)    # Use PCA model to decompose the sample set    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate The Multiple Linear Regression Model Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each coefficient in the above regression model: [[-2.37266325  2.70266357  3.47985379 -0.23317115 -1.5608932  -0.56799467\n",
      "  -0.21134992]]\n",
      "The intercept in the above regression model: [22.74548023]\n",
      "The R-square of the above regression model: 0.7310611528589397\n",
      "The adjusted R-square of the above regression model: 0.7207781969388404\n",
      "The p-value of each coefficient in the above regression model: [6.41158158e-14 4.95302857e-14 3.05628798e-24 2.17666529e-03\n",
      " 4.28001007e-17 2.44958160e-55 2.24917643e-14 2.90986820e-06\n",
      " 2.83596692e-14 6.28935737e-22 3.28375250e-31 4.03681568e-10\n",
      " 1.99312376e-63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "regression.fit(new_X_train_nor, y_train)   \n",
    "\n",
    "# Examine The Performance of Linear Regression Model\n",
    "r_squared = regression.score(new_X_train_nor, y_train)\n",
    "adj_r_squared = r_squared - (1 - r_squared) * (X_train.shape[1] / (X_train.shape[0] - X_train.shape[1] - 1))\n",
    "\n",
    "print('Each coefficient in the above regression model:', regression.coef_)\n",
    "print('The intercept in the above regression model:', regression.intercept_ )\n",
    "print('The R-square of the above regression model:', r_squared)\n",
    "print('The adjusted R-square of the above regression model:', adj_r_squared)\n",
    "print('The p-value of each coefficient in the above regression model:', f_regression(X_train, y_train)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
